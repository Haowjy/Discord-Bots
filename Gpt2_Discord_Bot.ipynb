{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Gpt2 Discord Bot.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "mount_file_id": "1puDMkHHsWxMcAkHcCuFgeiD1XWdxMywS",
      "authorship_tag": "ABX9TyNQVDen+z3lqTJHOz08SLva",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Haowjy/Discord-Bots/blob/main/Gpt2_Discord_Bot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5jZ0mFsyvzvI"
      },
      "source": [
        "# GPT-2 Discord Bot\n",
        "## Generate Text using gpt2 and host a discord bot in colab (for up to 12 hours at a time)\n",
        "Scraped and trained from RoyalRoad using: [colab link](https://colab.research.google.com/github/Haowjy/Discord-Bots/blob/main/Scrape_and_Train_Royalroad_Synopsis.ipynb)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nr904ujqIP0v",
        "outputId": "d47ac0dd-ce9b-43c0-925d-9aa266acb4fd"
      },
      "source": [
        "%tensorflow_version 1.x\n",
        "!pip install -q gpt-2-simple\n",
        "import gpt_2_simple as gpt2\n",
        "from datetime import datetime\n",
        "from google.colab import files"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 1.x selected.\n",
            "  Building wheel for gpt-2-simple (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "WARNING:tensorflow:\n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jb8Qd_aMSZtQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e76b752e-b6ed-4784-8322-e2688c488d10"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tgV1SR1rITc8",
        "outputId": "f3b6a3ce-03c7-4ebb-e6ef-aa4de84c6038"
      },
      "source": [
        "import tensorflow as tf\n",
        "tf.reset_default_graph()\n",
        "\n",
        "modelName = \"124M\"\n",
        "# modelName = \"355M\"\n",
        "# modelName = \"774M\"\n",
        "\n",
        "gpt2.download_gpt2(model_name=modelName)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fetching checkpoint: 1.05Mit [00:00, 89.2Mit/s]                                                     \n",
            "Fetching encoder.json: 1.05Mit [00:00, 5.30Mit/s]\n",
            "Fetching hparams.json: 1.05Mit [00:00, 225Mit/s]                                                    \n",
            "Fetching model.ckpt.data-00000-of-00001: 498Mit [00:41, 12.1Mit/s]\n",
            "Fetching model.ckpt.index: 1.05Mit [00:00, 316Mit/s]                                                \n",
            "Fetching model.ckpt.meta: 1.05Mit [00:00, 6.21Mit/s]\n",
            "Fetching vocab.bpe: 1.05Mit [00:00, 8.08Mit/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PHXjaQrRIZtT",
        "outputId": "28daa629-94a6-48b9-ee0a-384216d6bf71"
      },
      "source": [
        "sess = gpt2.start_tf_sess()\n",
        "\n",
        "gpt2.load_gpt2(sess, model_name=modelName)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading pretrained model models/124M/model.ckpt\n",
            "INFO:tensorflow:Restoring parameters from models/124M/model.ckpt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 242
        },
        "id": "gSlWA9HtIglk",
        "outputId": "547d3b8b-c79c-461c-de3e-f732c567664c"
      },
      "source": [
        "gpt2.generate(sess,\n",
        "              return_as_list=True,\n",
        "              model_name=modelName,\n",
        "              prefix=\"\",\n",
        "              length=250,\n",
        "              temperature=0.7,\n",
        "              top_p=0.9,\n",
        "              nsamples=1,\n",
        "              batch_size=1\n",
        "              )[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/gpt_2_simple/src/sample.py:32: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Sigh. I\\'ll just have to wait and see how things play out.\\n\\nAdvertisements<|endoftext|>MADISON, Wis. -- A Wisconsin man has been charged with murder in the death of a 13-year-old boy after he was shot by a group of teens in the Milwaukee area on Sunday.\\n\\nMadison police said the shooting happened around 11:30 p.m. in the 200 block of North N. N. Foye.\\n\\nPolice said the teen was sitting in the living room of his home when a group of teenagers began shooting at him.\\n\\n\"The guy got out of his car and started shooting at the kids. He shot one of them, but he was getting shot and he didn\\'t get out of his car,\" Milwaukee Police Chief Edward Flynn said.\\n\\nThe man was taken to the hospital in stable condition. He was later pronounced dead at the scene.\\n\\nFlynn said the investigation is continuing.\\n\\nAnyone with information is asked to call the Homicide Unit at (414) 497-8343.\\n\\nCopyright 2015 by ClickOnDetroit.com. All rights reserved. This material may not be published, broadcast, rewritten or redistributed.<|endoftext|>We are pleased to'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 171
        },
        "id": "Myxe3i9W0aHc",
        "outputId": "edf61237-5c8d-4de0-90d6-4dbb154a2976"
      },
      "source": [
        "gpt2.generate(sess,\n",
        "              return_as_list=True,\n",
        "              model_name=modelName,\n",
        "              prefix=\"Me: What is your name?\\nJason:\",\n",
        "              length=250,\n",
        "              temperature=0.7,\n",
        "              top_p=0.9,\n",
        "              nsamples=1,\n",
        "              batch_size=1\n",
        "              )[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"Me: What is your name?\\nJason: My name is Jason, and I'm a short guy, and I'm the CEO of T-Mobile.\\nT-Mobile's brand, T-Mobile's new store in New York City, is named after a man who's been living in New York City since 1979. He lives in a Manhattan apartment.\\nT-Mobile's new store in New York City, is named after a man who's been living in New York City since 1979. He lives in a Manhattan apartment.\\nT-Mobile's new store in New York City, is named after a man who's been living in New York City since 1979. He lives in a Manhattan apartment.\\nT-Mobile's new store in New York City, is named after a man who's been living in New York City since 1979. He lives in a Manhattan apartment.\\nT-Mobile's new store in New York City, is named after a man who's been living in New York City since 1979. He lives in a Manhattan apartment.\\nT-Mobile's new store in New York City, is named after a man who's been living in New York City since 1979. He lives in a Manhattan apartment.\\nT-Mobile's new store in New York City,\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gf31Vr17R2Py",
        "outputId": "6294423e-cefa-4d9e-c62b-65eec12a01f3"
      },
      "source": [
        "!pip install nest_asyncio\n",
        "!pip install discord"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: nest_asyncio in /usr/local/lib/python3.7/dist-packages (1.5.1)\n",
            "Collecting discord\n",
            "  Downloading https://files.pythonhosted.org/packages/aa/76/75382e581c7932c177568e985a6967a305b1788f51936e819010e5538ef4/discord-1.0.1-py3-none-any.whl\n",
            "Collecting discord.py>=1.0.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/62/9d/d5d9134bf894cce5b1acc698222d2b80c51f017b17e4211f4f33cebffccb/discord.py-1.6.0-py3-none-any.whl (779kB)\n",
            "\u001b[K     |████████████████████████████████| 788kB 7.5MB/s \n",
            "\u001b[?25hCollecting aiohttp<3.8.0,>=3.6.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5b/a6/d36302eba284f4f427dc288f6b3ecd7f89d739cfca206b80311d3158f6d9/aiohttp-3.7.4-cp37-cp37m-manylinux2014_x86_64.whl (1.3MB)\n",
            "\u001b[K     |████████████████████████████████| 1.3MB 15.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp<3.8.0,>=3.6.0->discord.py>=1.0.1->discord) (20.3.0)\n",
            "Collecting multidict<7.0,>=4.5\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7c/a6/4123b8165acbe773d1a8dc8e3f0d1edea16d29f7de018eda769abb56bd30/multidict-5.1.0-cp37-cp37m-manylinux2014_x86_64.whl (142kB)\n",
            "\u001b[K     |████████████████████████████████| 143kB 38.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.6.5 in /usr/local/lib/python3.7/dist-packages (from aiohttp<3.8.0,>=3.6.0->discord.py>=1.0.1->discord) (3.7.4.3)\n",
            "Collecting yarl<2.0,>=1.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f1/62/046834c5fc998c88ab2ef722f5d42122230a632212c8afa76418324f53ff/yarl-1.6.3-cp37-cp37m-manylinux2014_x86_64.whl (294kB)\n",
            "\u001b[K     |████████████████████████████████| 296kB 33.5MB/s \n",
            "\u001b[?25hCollecting async-timeout<4.0,>=3.0\n",
            "  Downloading https://files.pythonhosted.org/packages/e1/1e/5a4441be21b0726c4464f3f23c8b19628372f606755a9d2e46c187e65ec4/async_timeout-3.0.1-py3-none-any.whl\n",
            "Requirement already satisfied: chardet<4.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp<3.8.0,>=3.6.0->discord.py>=1.0.1->discord) (3.0.4)\n",
            "Requirement already satisfied: idna>=2.0 in /usr/local/lib/python3.7/dist-packages (from yarl<2.0,>=1.0->aiohttp<3.8.0,>=3.6.0->discord.py>=1.0.1->discord) (2.10)\n",
            "Installing collected packages: multidict, yarl, async-timeout, aiohttp, discord.py, discord\n",
            "Successfully installed aiohttp-3.7.4 async-timeout-3.0.1 discord-1.0.1 discord.py-1.6.0 multidict-5.1.0 yarl-1.6.3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C9_vgc0IRBv8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 514
        },
        "outputId": "a92b465e-f538-4a9e-fb29-eafedad5cf46"
      },
      "source": [
        "import asyncio\n",
        "import nest_asyncio\n",
        "import re\n",
        "\n",
        "nest_asyncio.apply()\n",
        "\n",
        "import discord\n",
        "from discord.ext import commands\n",
        "\n",
        "intents = discord.Intents.default()\n",
        "intents.typing = True\n",
        "intents.presences = False\n",
        "\n",
        "# Client (our bot)\n",
        "#client = discord.Client()\n",
        "client = commands.Bot(command_prefix=\"!\", help_command=None, intents=intents)\n",
        "\n",
        "@client.command(name=\"version\")\n",
        "async def _version(context):\n",
        "    \n",
        "    myEmbed = discord.Embed(title=\"Current Version\", description=\"Version 1.0\", color=0x00ff00)\n",
        "    myEmbed.add_field(name=\"Version Code:\", value=\"v1.0.0\", inline=False)\n",
        "    myEmbed.add_field(name=\"Model Size:\", value=\"355M\", inline=True)\n",
        "    myEmbed.add_field(name=\"Retrained:\", value=\"False\", inline=True)\n",
        "    myEmbed.add_field(name=\"Date Released:\", value=\"January 8th, 2021\", inline=False)\n",
        "    myEmbed.set_footer(text=\"by xerluc\")\n",
        "    await context.message.channel.send(\"!\", embed=myEmbed)\n",
        "\n",
        "@client.command(name=\"about\")\n",
        "async def _about(context):\n",
        "    \n",
        "    myEmbed = discord.Embed(title=\"About GPT2\", description=\"created by OpenAI\", color=0x87ceeb)\n",
        "    await context.message.channel.send(\"!\", embed=myEmbed)\n",
        "    await context.message.channel.send(\"> https://openai.com/blog/better-language-models/\")\n",
        "    await context.message.channel.send(\"> GPT-2 is a large transformer-based language model with 1.5 billion parameters, trained on a dataset of 8 million web pages. GPT-2 is trained with a simple objective: predict the next word, given all of the previous words within some text. The diversity of the dataset causes this simple goal to contain naturally occurring demonstrations of many tasks across diverse domains. GPT-2 is a direct scale-up of GPT, with more than 10X the parameters and trained on more than 10X the amount of data.\")\n",
        "    await context.message.channel.send(\"> https://en.wikipedia.org/wiki/GPT-2\")\n",
        "\n",
        "    myEmbed = discord.Embed(color=0x00ff00)\n",
        "    myEmbed.add_field(name=\"Current Model Size:\", value=\"355M\", inline=True)\n",
        "    myEmbed.add_field(name=\"Retrained:\", value=\"False\", inline=True)\n",
        "    await context.message.channel.send(embed=myEmbed)\n",
        "\n",
        "@client.command(name=\"help\")\n",
        "async def _help(context):\n",
        "    myEmbed = discord.Embed(title=\"GPT2-bot Help\", description=\" \", color=0x87ceeb)\n",
        "    myEmbed.add_field(name=\"```!help```\", value=\"display this message\", inline=False)\n",
        "    myEmbed.add_field(name=\"```!gpt2 [optional field]```\", value=\"Have gpt2 (335M) generate text! Either leave the optional field blank or give it a prompt, and gpt2 will build about 2 paragraphs for you!\", inline=False)\n",
        "    myEmbed.add_field(name=\"```!version```\", value=\"Display version information\", inline=False)\n",
        "    myEmbed.add_field(name=\"```!about```\", value=\"About GPT-2\", inline=False)\n",
        "    myEmbed.add_field(name=\"```[anything else]```\", value=\"runs the !gpt2 command on everything else\", inline=False)\n",
        "    await context.message.channel.send(\"!\", embed=myEmbed)\n",
        "\n",
        "    myEmbed = discord.Embed(title=\"Examples\", description=\" \", color=0x87ceeb)\n",
        "    myEmbed.add_field(name=\"\\u200b\", value=\"```!gpt2```\", inline=False)\n",
        "    myEmbed.add_field(name=\"\\u200b\", value=\"```!gpt2 \\\"There was a poll that asked me\\\"```\", inline=False)\n",
        "    myEmbed.add_field(name=\"\\u200b\", value=\"```!gpt2 There was a poll that asked me```\", inline=False)\n",
        "    myEmbed.add_field(name=\"\\u200b\", value=\"```There was a poll that asked me```\", inline=False)\n",
        "    await context.message.channel.send(embed=myEmbed)\n",
        "\n",
        "    #await context.message.channel.send(\"> Examples: \\n> ```!gpt2```\\n> ```!gpt2 There was a poll that asked me```\\n> ```!gpt2 \\\"There was a poll that asked me\\\"```\\n> ```There was a poll that asked me```\")\n",
        "\n",
        "def generate(input):\n",
        "    return gpt2.generate(sess,\n",
        "              return_as_list=True,\n",
        "              model_name=modelName,\n",
        "              prefix=input,\n",
        "              length=200,\n",
        "              truncate='<|endoftext|>',\n",
        "              temperature=0.7,\n",
        "              top_p=0.9,\n",
        "              nsamples=1,\n",
        "              batch_size=1\n",
        "              )[0]\n",
        "\n",
        "@client.command(name=\"gpt2\")\n",
        "async def _gpt2(context, *args):\n",
        "    print(\"running gpt2...\")\n",
        "    if(args==None):\n",
        "        arg = \"\"\n",
        "    else:\n",
        "        arg = ' '.join(args)\n",
        "    response = generate(arg)\n",
        "    split = re.split(\"\\n\",response)\n",
        "    redone = \"> \"+split[0]\n",
        "    for i in split[1:]:\n",
        "        redone+=\"\\n> \"+i\n",
        "    # myEmbed = discord.Embed(title=\"GPT2-Bot\", description=response, color=0x87ceeb)\n",
        "    # print(response)\n",
        "    # print(\"\\nsuccess!\")\n",
        "    await context.message.channel.send(redone, tts=False) #m must be at the end\n",
        "\n",
        "@client.event\n",
        "async def on_ready():\n",
        "\n",
        "    #gpt2_channel = client.get_channel(797925137356947456)\n",
        "    gpt2_channel = client.get_channel(816803978745741366)\n",
        "    816803978745741366\n",
        "    myEmbed = discord.Embed(title=\"GPT2-bot\", description=\"GPT2-bot is now online!\", color=0x87ceeb)\n",
        "    await gpt2_channel.send(\"@everyone\", embed=myEmbed)\n",
        "\n",
        "@client.event\n",
        "async def on_message(message):\n",
        "    if message.author == client.user:\n",
        "        return\n",
        "\n",
        "    if((not message.content.startswith(\"!\")) and (not message.content.startswith(\"?\")) and (not message.author.id == 799466415665053726)):\n",
        "        response = generate(message.content)\n",
        "        split = re.split(\"\\n\",response)\n",
        "        redone = \"> \"+split[0]\n",
        "        for i in split[1:]:\n",
        "            redone+=\"\\n> \"+i\n",
        "        #myEmbed = discord.Embed(title=\"GPT2-Bot\", description=response, color=0x87ceeb)\n",
        "        await message.channel.send(redone, tts=False)\n",
        "\n",
        "    await client.process_commands(message)\n",
        "\n",
        "# Run the client from google colab\n",
        "f = open(\"/content/drive/MyDrive/Discord Tokens/gpt2.txt\")\n",
        "discord_token = f.readline()\n",
        "f.close()\n",
        "\n",
        "async def run():\n",
        "    r = await client.run(discord_token)\n",
        "    return r\n",
        "\n",
        "discordbot = asyncio.run(run())\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "running gpt2...\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/gpt_2_simple/src/sample.py:32: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/discord/client.py\u001b[0m in \u001b[0;36m_cleanup_loop\u001b[0;34m(loop)\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         \u001b[0m_cancel_tasks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversion_info\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/discord/client.py\u001b[0m in \u001b[0;36m_cancel_tasks\u001b[0;34m(loop)\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m     \u001b[0mloop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_until_complete\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0masyncio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgather\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_exceptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m     \u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'All tasks finished cancelling.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/nest_asyncio.py\u001b[0m in \u001b[0;36mrun_until_complete\u001b[0;34m(self, future)\u001b[0m\n\u001b[1;32m     68\u001b[0m                 raise RuntimeError(\n\u001b[0;32m---> 69\u001b[0;31m                     'Event loop stopped before Future completed.')\n\u001b[0m\u001b[1;32m     70\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Event loop stopped before Future completed.",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-ce0131f63751>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    124\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m \u001b[0mdiscordbot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0masyncio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/nest_asyncio.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(future, debug)\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0mloop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0masyncio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_event_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mloop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_debug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mloop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_until_complete\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfuture\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversion_info\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/nest_asyncio.py\u001b[0m in \u001b[0;36mrun_until_complete\u001b[0;34m(self, future)\u001b[0m\n\u001b[1;32m     68\u001b[0m                 raise RuntimeError(\n\u001b[1;32m     69\u001b[0m                     'Event loop stopped before Future completed.')\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_once\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/asyncio/futures.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    179\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__log_traceback\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 181\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    182\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/asyncio/tasks.py\u001b[0m in \u001b[0;36m__step\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    247\u001b[0m                 \u001b[0;31m# We use the `send` method directly, because coroutines\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    248\u001b[0m                 \u001b[0;31m# don't have `__iter__` and `__next__` methods.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 249\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcoro\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    250\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcoro\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mthrow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-9-ce0131f63751>\u001b[0m in \u001b[0;36mrun\u001b[0;34m()\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[0;32masync\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m     \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mawait\u001b[0m \u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdiscord_token\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    124\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/discord/client.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    712\u001b[0m             \u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremove_done_callback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstop_loop_on_completion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    713\u001b[0m             \u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Cleaning up tasks.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 714\u001b[0;31m             \u001b[0m_cleanup_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    715\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    716\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcancelled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/discord/client.py\u001b[0m in \u001b[0;36m_cleanup_loop\u001b[0;34m(loop)\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Closing the event loop.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m         \u001b[0mloop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0m_ClientEventTask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0masyncio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/asyncio/unix_events.py\u001b[0m in \u001b[0;36mclose\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_finalizing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0msig\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_signal_handlers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/asyncio/selector_events.py\u001b[0m in \u001b[0;36mclose\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_running\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Cannot close a running event loop\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_closed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Cannot close a running event loop"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WI7CGe_mLUTL"
      },
      "source": [
        "#Flash App"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CX6URqlVNUnI"
      },
      "source": [
        "# !pip install flask-ngrok\n",
        "# !pip install flask==0.12.2  # Newer versions of flask don't work in Colab\n",
        "#                             # See https://github.com/plotly/dash/issues/257"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "quuAMhiONRd2"
      },
      "source": [
        "# # flask_ngrok_example.py\n",
        "# from flask import Flask\n",
        "# from flask_ngrok import run_with_ngrok\n",
        "\n",
        "# app = Flask(__name__)\n",
        "# run_with_ngrok(app)  # Start ngrok when app is run\n",
        "\n",
        "# @app.route(\"/\")\n",
        "# def hello():\n",
        "#     return \"Hello World!\"\n",
        "\n",
        "# if __name__ == '__main__':\n",
        "#     app.run()  # If address is in use, may need to terminate other sessions:\n",
        "#                # Runtime > Manage Sessions > Terminate Other Sessions"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}